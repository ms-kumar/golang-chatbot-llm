## Download tinyllama llm model locally
1. curl -fsSL https://ollama.com/install.sh | sh
2. ollama pull tinyllama

## Download the following
1. go get -u github.com/spf13/cobra@latest
2. go install github.com/spf13/cobra-cli@latest

3. go get github.com/tmc/langchaingo/llms/ollama
4. go get github.com/tmc/langchaingo/llms

## Run cli-inference locally
go run main.go llm